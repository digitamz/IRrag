{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.7445025899612032)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.7445025899612032)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ["Takeo Miki (1908-1988) was a prominent Japanese politician who had a long and influential career in Japanese politics. Here is a brief biography:\n\n- **Early Life and Education**: Born in Awa, Tokushima, Japan, in 1908, Miki pursued higher education at Meiji University in Tokyo. He later attended the University of Southern California in Los Angeles, where he was awarded an honorary doctorate in law in 1966.\n\n- **Political Career**: Miki was first elected to the Diet (Japan's national legislature) in 1937. He continued to serve in this capacity until his death in 1988, representing his constituency for an unprecedented 19 terms over 51 years. His political resilience was demonstrated in the 1942 general election, where he ran in opposition to the military government under Hideki Tojo and managed to win despite the challenging political climate.\n\n- **Prime Minister of Japan**: Miki served as Prime Minister of Japan from 1974 to 1976. However, his tenure was marked by a difficult period, including a coup d'état attempt within his own party. Despite these challenges, he held multiple other significant posts throughout his career.\n\n- **International Relations**: In 1976, to commemorate the United States Bicentennial, Miki gifted 1,000 cherry trees to Seattle, strengthening the ties between Japan and the United States. This gesture is still celebrated annually in Seattle.\n\n- **Cultural Impact**: In Hong Kong, Miki's name (三木武夫) has been used colloquially to describe actors or actresses with wooden or emotionless expressions, a reference to his own often stoic demeanor in news reports.\n\n- **Legacy**: Takeo Miki's long-standing service to the Diet and his role as Prime Minister have left a significant mark on Japanese political history. His contributions to international relations and cultural exchange have also been notable.\n\nTakeo Miki's life and career reflect the complexities and challenges of Japanese politics in the 20th century, as well as his enduring impact on both domestic and international affairs."], 'claims': [['Miki was born in 1908 in Awa, Tokushima, Japan.', 'Miki pursued higher education at Meiji University in Tokyo.', 'Miki attended the University of Southern California in Los Angeles.', 'Miki was awarded an honorary doctorate in law from the University of Southern California in 1966.', 'Miki was first elected to the Diet in 1937.', 'Miki served in the Diet until his death in 1988.', 'Miki represented his constituency for 19 terms over 51 years.', 'Miki ran in opposition to the military government under Hideki Tojo in the 1942 general election.', 'Miki won the 1942 general election despite the challenging political climate.', 'Miki served as Prime Minister of Japan from 1974 to 1976.', "Miki's tenure as Prime Minister of Japan was marked by a difficult period.", "Miki's tenure as Prime Minister of Japan included a coup d'état attempt within his own party.", 'Miki held multiple other significant posts throughout his career.', 'Miki gifted 1,000 cherry trees to Seattle in 1976 to commemorate the United States Bicentennial.', 'This gesture strengthened the ties between Japan and the United States.', 'This gesture is still celebrated annually in Seattle.', "Miki's name (三木武夫) has been used colloquially in Hong Kong to describe actors or actresses with wooden or emotionless expressions.", "Miki's stoic demeanor in news reports is the reference for this colloquial usage.", "Takeo Miki's long-standing service to the Diet has left a significant mark on Japanese political history.", "Takeo Miki's role as Prime Minister has left a significant mark on Japanese political history.", "Takeo Miki's contributions to international relations have been notable.", "Takeo Miki's contributions to cultural exchange have been notable.", "Takeo Miki's life and career reflect the complexities and challenges of Japanese politics in the 20th century.", 'Takeo Miki had an enduring impact on both domestic and international affairs.']], 'claim_correctness': [array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1, -1,  1,  1,
        1,  1,  0,  1,  1,  1, -1])], 'question_text': ['Tell me a bio of Takeo Miki.'], 'context': ['{\n  "id" : "10638876",\n  "contents" : "\\"Takeo Miki\\"\\nTakeo Miki Born in Awa, Tokushima, Miki graduated from Meiji University in Tokyo. He attended the University of Southern California in Los Angeles, and was awarded an honorary doctorate in law from the institution in 1966. Miki was elected to the Diet in 1937 and remained there until his death in 1988, during which he was elected a representative for 19 times over 51 years. In the 1942 general election he ran in opposition to the military government under Hideki Tojo and still managed to win a seat; his efforts at this time were assisted by Kan Abe, the grandfather"\n}\n{\n  "id" : "16033956",\n  "contents" : "\\"Takeo Miki\\"\\ncondition, as Miki was suffering a coup d\'état from amongst his own party. Mao had never shown any interest in Miki before, or even mentioned him. He held many other posts during his career in addition to being prime minister. NFL player Takeo Spikes was named after Miki. To commemorate the ties of Japan to America, and Seattle in particular, Miki gave 1,000 cherry trees to Seattle to commemorate the United States Bicentennial in 1976. This gift gave birth to the , still running annually. In Hong Kong, the name \\"\\"Takeo Miki\\"\\" (三木武夫) is sometimes used to describe actors or"\n}\n{\n  "id" : "3620139",\n  "contents" : "\\"Takeo Miki\\"\\nactresses with wooden or no emotional expressions during movies or TV dramas. Some have said that the origin for the slang term stems from Miki\'s wooden expression during his appearance in news reports. Takeo Miki Born in Awa, Tokushima, Miki graduated from Meiji University in Tokyo. He attended the University of Southern California in Los Angeles, and was awarded an honorary doctorate in law from the institution in 1966. Miki was elected to the Diet in 1937 and remained there until his death in 1988, during which he was elected a representative for 19 times over 51 years. In the"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x0000019B1CFFC880>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x0000019B1D04D3C0>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000019B1CFFFF70>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000019B1D06A770>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.06528727054595947), np.float64(-0.00819598736454158)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.046257638931274415), np.float64(-2.220446049250313e-16)], [np.float64(-0.08100440979003906), np.float64(-0.00623903660519326)], [np.float64(-0.29950575828552245), np.float64(-2.826395193864424)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.131821928024292), np.float64(-1.788854381999831)], [np.float64(-0.06036465883255005), np.float64(-0.0056367036295430495)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.2411274194717407), np.float64(-3.0100237091494413)], [np.float64(-0.22498152971267701), np.float64(-2.555531630203088)], [np.float64(-0.0905165719985962), np.float64(-0.03486620484220998)], [np.float64(-0.21038010120391845), np.float64(-2.6343052975244694)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.11169380187988281), np.float64(-0.037516616611132036)], [np.float64(-0.054492881298065184), np.float64(-0.003489010646041013)], [np.float64(-0.08164013862609863), np.float64(-0.011845373812482296)], [np.float64(-10000000000.0), np.float64(-10000000000.0)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.float64(0.48368397744070724), np.float64(0.497951014628766)], [np.int64(0), np.int64(0)], [np.float64(0.48843765192331023), np.float64(0.49999999999999994)], [np.float64(0.47975996378708785), np.float64(0.49844024590822583)], [np.float64(0.42567830912646987), np.float64(0.05591438396759551)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.46709215755338335), np.float64(0.14321323703222522)], [np.float64(0.48491341617055483), np.float64(0.49859082782368075)], [np.int64(0), np.int64(0)], [np.float64(0.4400085346595805), np.float64(0.0469750842455876)], [np.float64(0.44399066908036844), np.float64(0.07205574334009024)], [np.float64(0.47738629487030015), np.float64(0.4912843317066994)], [np.float64(0.44759810699595265), np.float64(0.06696295823357527)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.47210554328909676), np.float64(0.49062194578630525)], [np.float64(0.4863801498247366), np.float64(0.4991277482233305)], [np.float64(0.4796012940571291), np.float64(0.49703869117261285)], [np.int64(0), np.int64(0)]]]}}}