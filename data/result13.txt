{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.09690568374366491)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.09690568374366491)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ['Peter Killian Gallagher, born on August 19, 1955, in Manhattan, New York City, is an American actor, musician, and writer. He comes from a family with a strong academic and professional background; his mother, Mary Ann O\'Shea, was a bacteriologist, and his father, Thomas Francis Gallagher, Jr., had a notable career as well.\n\nGallagher\'s acting career began in 1980, and he has since become a well-known figure in Hollywood, starring in various films and television series. He is most recognized for his role as Sandy Cohen in the popular television drama series "The O.C.," which aired from 2003 to 2007. In this series, he portrayed a lawyer and father who becomes a guardian to the protagonist, Ryan Atwood.\n\nBeyond "The O.C.," Gallagher has had a recurring role as Deputy Chief William Dodds on a notable television series (the specific show is not mentioned in the provided context). He has also ventured into the music industry and has been involved in writing.\n\nIn 2015, Gallagher lent his voice to Martha in Deaf West Theatre\'s Broadway production of "Spring Awakening," showcasing his versatility as an artist.\n\nHis contributions to both film and television, as well as his other artistic pursuits, have solidified his place in the entertainment industry.'], 'claims': [['Peter Killian Gallagher was born on August 19, 1955.', 'Peter Killian Gallagher was born in Manhattan, New York City.', 'Peter Killian Gallagher is an American actor, musician, and writer.', "Mary Ann O'Shea was Peter Killian Gallagher's mother.", "Mary Ann O'Shea was a bacteriologist.", "Thomas Francis Gallagher, Jr. was Peter Killian Gallagher's father.", 'Thomas Francis Gallagher, Jr. had a notable career.', "Gallagher's acting career began in 1980.", 'Gallagher has become a well-known figure in Hollywood.', 'Gallagher has starred in various films and television series.', "Gallagher is most recognized for his role as Sandy Cohen in the television drama series 'The O.C.'.", "'The O.C.' aired from 2003 to 2007.", "In 'The O.C.', Gallagher portrayed a lawyer and father who becomes a guardian to the protagonist, Ryan Atwood.", 'Gallagher has had a recurring role as Deputy Chief William Dodds on a notable television series.', 'Gallagher has ventured into the music industry.', 'Gallagher has been involved in writing.', "Gallagher lent his voice to Martha in Deaf West Theatre's Broadway production of 'Spring Awakening' in 2015.", 'Gallagher showcased his versatility as an artist in 2015.', 'His contributions to both film and television have solidified his place in the entertainment industry.', 'His contributions to other artistic pursuits have solidified his place in the entertainment industry.']], 'claim_correctness': [array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], 'question_text': ['Tell me a bio of Peter Gallagher.'], 'context': ['{\n  "id" : "14793136",\n  "contents" : "\\"Peter Gallagher\\"\\nPeter Gallagher Peter Killian Gallagher (born August 19, 1955) is an American actor, musician and writer. Since 1980, Gallagher has played roles in numerous Hollywood films. He is best known for starring as Sandy Cohen in the television drama series \\"\\"The O.C.\\"\\" from 2003 to 2007, and a recurring role as Deputy Chief William Dodds on \\"\\"\\"\\". Gallagher was born in the Manhattan borough of New York City. His mother, Mary Ann (née O\'Shea; August 23, 1915 - June 6, 2004), was a bacteriologist, and his father, Thomas Francis Gallagher, Jr. (June 10, 1912 - November 16, 1999), was an"\n}\n{\n  "id" : "8474070",\n  "contents" : "\\"Peter Gallagher\\"\\nand singer, and in 2015 performed in Deaf West Theatre\'s production of \\"\\"Spring Awakening\\"\\" on Broadway as the voice of Martha. Peter Gallagher Peter Killian Gallagher (born August 19, 1955) is an American actor, musician and writer. Since 1980, Gallagher has played roles in numerous Hollywood films. He is best known for starring as Sandy Cohen in the television drama series \\"\\"The O.C.\\"\\" from 2003 to 2007, and a recurring role as Deputy Chief William Dodds on \\"\\"\\"\\". Gallagher was born in the Manhattan borough of New York City. His mother, Mary Ann (née O\'Shea; August 23, 1915 - June"\n}\n{\n  "id" : "9046982",\n  "contents" : "\\"Peter Gallagher (rugby league, born 1937)\\"\\nPeter Gallagher (rugby league, born 1937) Peter \\"\\"Pedro\\"\\" Gallagher (1937–2003) was an Australian rugby league footballer. He was a front-row forward for the Australian national team. He played in 17 Tests between 1963 and 1968 as captain on 1 occasion. He is considered one of the nation\'s finest footballers of the 20th century. Born in Townsville, Queensland Gallagher played his entire Brisbane Rugby League premiership first grade career of 11 seasons with the Brothers club. He first represented for Queensland at age 25 in 1962 and then regularly over the next 5 years making 12 appearances against New South Wales"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x0000020BB35CE8C0>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x0000020BB38793F0>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000020BB381FF10>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000020BB388A740>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.7184750747680664), np.float64(-4.47213595499958)], [np.float64(-0.42169788122177126), np.float64(-3.7889581816614526)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.16530383110046387), np.float64(-1.965058739848726)], [np.float64(-0.1878650164604187), np.float64(-1.9799609798019335)], [np.float64(-0.042119927406311035), np.float64(-0.0030921691107842486)], [np.float64(-0.03884024620056152), np.float64(-0.00021161034899230824)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.04258108139038086), np.float64(-3.3306690738754696e-16)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.05578410148620606), np.float64(-0.0073412419665749296)], [np.float64(-0.04901022911071777), np.float64(-1.1102230246251565e-16)], [np.float64(-0.5579746675491333), np.float64(-4.472135954999581)], [np.float64(-0.04453007459640503), np.float64(-0.0026984161542228935)], [np.float64(-0.051412830352783205), np.float64(-0.009592765928796487)], [np.float64(-0.04281300067901611), np.float64(-0.000972245603405919)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.float64(0.3277288702094715), np.float64(0.011293882208110636)], [np.float64(0.39611053354216197), np.float64(0.02211884505371705)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.45876788966940757), np.float64(0.12292061788255705)], [np.float64(0.45317139280644686), np.float64(0.12132299752884668)], [np.float64(0.48947157463207563), np.float64(0.49922695833825764)], [np.float64(0.4902911589537666), np.float64(0.49994709741294935)], [np.int64(0), np.int64(0)], [np.float64(0.4893563378154783), np.float64(0.49999999999999994)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.48605759001688315), np.float64(0.4981646977509717)], [np.float64(0.48774989468951885), np.float64(0.49999999999999994)], [np.float64(0.3640162119522143), np.float64(0.011293882208110617)], [np.float64(0.48886932056768445), np.float64(0.49932539637078527)], [np.float64(0.48714962288157654), np.float64(0.4976018269079948)], [np.float64(0.4892983844102024), np.float64(0.4997569386182949)]]]}}}