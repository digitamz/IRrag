{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.874134485762254)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.874134485762254)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ['Valerie Kathryn Harper, born on August 22, 1939, is an accomplished American actress known for her versatile talent across television, film, and stage. Her most iconic role came in the 1970s when she played Rhoda Morgenstern on "The Mary Tyler Moore Show" and its spin-off, "Rhoda." Her performance in these series earned her four Primetime Emmy Awards and established her as a significant figure in television.\n\nHarper\'s acting career began on Broadway, where she made her debut as a dancer in the musical "Take Her, She\'s Mine." Her breakthrough came with her role in the Broadway production of "Li\'l Abner" before transitioning to television. In addition to her Emmy wins, she has been nominated for eight Emmy Awards and six Golden Globe Awards, winning one Golden Globe.\n\nHer notable film roles include "Freebie and the Bean" (1974) and "Chapter Two" (1979), both of which earned her Golden Globe nominations. On stage, Harper has appeared in several Broadway productions, and she started her career as a dancer, which contributed to her later success in both musicals and straight plays.\n\nPersonal details of Harper\'s life reveal a complex family background. She has a half-sister, Virginia, from her father\'s second marriage. Her parents originally expected a boy, and her first and middle names, Valerie and Kathryn, were inspired by the women\'s doubles tennis champions of the year, Valerie Scott and Kay Stammers. She is of French, English, Irish, Scottish, and Welsh ancestry. Harper was raised Catholic but distanced herself from the religion at an early age. Her family moved frequently, which influenced her early life and career.\n\nHarper based her character Rhoda Morgenstern on her Italian stepmother, Angela Posillico, and Penny Ann Green (née Joanna Greenberg), with whom she danced in the Broadway musical "Wildcat." This personal connection to her character added depth and authenticity to her performances.\n\nThroughout her career, Harper has been recognized for her talent and contributions to the entertainment industry, making her a beloved and respected figure in American television and film.'], 'claims': [['Valerie Kathryn Harper was born on August 22, 1939.', 'Valerie Kathryn Harper is an American actress.', 'Valerie Kathryn Harper is known for her versatile talent across television, film, and stage.', "Valerie Kathryn Harper played Rhoda Morgenstern on 'The Mary Tyler Moore Show'.", "Valerie Kathryn Harper played Rhoda Morgenstern on the spin-off 'Rhoda'.", "Valerie Kathryn Harper's performance as Rhoda Morgenstern earned her four Primetime Emmy Awards.", "Valerie Kathryn Harper's performance as Rhoda Morgenstern established her as a significant figure in television.", "Harper's acting career began on Broadway.", "Harper made her debut as a dancer in the musical 'Take Her, She's Mine.'", "Harper's breakthrough came with her role in the Broadway production of 'Li'l Abner.'", 'Harper has transitioned to television.', 'Harper has won Emmy Awards.', 'Harper has been nominated for eight Emmy Awards.', 'Harper has been nominated for six Golden Globe Awards.', 'Harper has won one Golden Globe Award.', "Her notable film roles include 'Freebie and the Bean' (1974).", "Her notable film roles include 'Chapter Two' (1979).", "Both 'Freebie and the Bean' (1974) and 'Chapter Two' (1979) earned her Golden Globe nominations.", 'Harper has appeared in several Broadway productions.', 'Harper started her career as a dancer.', 'Her career as a dancer contributed to her later success in both musicals and straight plays.', "Harper has a half-sister named Virginia from her father's second marriage.", "Harper's parents originally expected a boy.", "Harper's first name is Valerie and her middle name is Kathryn.", "Harper's first and middle names, Valerie and Kathryn, were inspired by the women's doubles tennis champions of the year, Valerie Scott and Kay Stammers.", 'Harper is of French, English, Irish, Scottish, and Welsh ancestry.', 'Harper was raised Catholic but distanced herself from the religion at an early age.', "Harper's family moved frequently, which influenced her early life and career.", 'Harper based her character Rhoda Morgenstern on her Italian stepmother, Angela Posillico.', 'Harper based her character Rhoda Morgenstern on Penny Ann Green (née Joanna Greenberg).', "Harper danced with Penny Ann Green (née Joanna Greenberg) in the Broadway musical 'Wildcat.'", "The personal connection to her character added depth and authenticity to Harper's performances.", 'Harper has been recognized for her talent and contributions to the entertainment industry.', 'Harper is a beloved and respected figure in American television and film.']], 'claim_correctness': [array([ 1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1])], 'question_text': ['Tell me a bio of Valerie Harper.'], 'context': ['{\n  "id" : "11336310",\n  "contents" : "\\"Valerie Harper\\"\\nValerie Harper Valerie Kathryn Harper (born August 22, 1939) is an American actress. She played Rhoda Morgenstern in the 1970s television series \\"\\"The Mary Tyler Moore Show\\"\\" and in its spin-off, \\"\\"Rhoda\\"\\". She later played Valerie Hogan in \\"\\"Valerie\\"\\". She is a four-time Primetime Emmy Award winner. Her notable film roles include \\"\\"Freebie and the Bean\\"\\" (1974), and \\"\\"Chapter Two\\"\\" (1979), both of which garnered her Golden Globe Award nominations. Harper is also known for her stage work, having appeared in several Broadway productions. She started her career as a dancer on Broadway, making her debut in the musical \\"\\"Take"\n}\n{\n  "id" : "11336325",\n  "contents" : "\\"Valerie Harper\\"\\nTony nominee, eight-time Emmy nominee (four wins) and a six-time Golden Globe nominee (one win). Valerie Harper Valerie Kathryn Harper (born August 22, 1939) is an American actress. She played Rhoda Morgenstern in the 1970s television series \\"\\"The Mary Tyler Moore Show\\"\\" and in its spin-off, \\"\\"Rhoda\\"\\". She later played Valerie Hogan in \\"\\"Valerie\\"\\". She is a four-time Primetime Emmy Award winner. Her notable film roles include \\"\\"Freebie and the Bean\\"\\" (1974), and \\"\\"Chapter Two\\"\\" (1979), both of which garnered her Golden Globe Award nominations. Harper is also known for her stage work, having appeared in several Broadway productions. She"\n}\n{\n  "id" : "11336312",\n  "contents" : "\\"Valerie Harper\\"\\nand a half-sister, Virginia, from her father\'s second marriage. She has stated that her parents were expecting a boy and after her arrival, her first and middle names derived from that year\'s women\'s doubles tennis champions, Valerie Scott and Kay Stammers. She is of French, English, Irish, Scottish, and Welsh ancestry. Harper based her character Rhoda Morgenstern on her Italian stepmother, Angela Posillico, and Penny Ann Green (née Joanna Greenberg), with whom she danced in the Broadway musical \\"\\"Wildcat\\"\\". She was raised Catholic, although at an early age she \\"\\"quit\\"\\" the church. The family moved every two years due to"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x000002513AEF4BE0>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x000002513B109480>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x000002513B0B3F10>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x000002513B11A6B0>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-0.069390869140625), np.float64(-0.01852698960730248)], [np.float64(-0.0417741322517395), np.float64(-8.318691164466152e-06)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.039874935150146486), np.float64(-2.7755575615628914e-16)], [np.float64(-0.03960824012756348), np.float64(-2.7755575615628914e-16)], [np.float64(-0.10472960710525513), np.float64(-0.0032550901117244235)], [np.float64(-0.036766393184661864), np.float64(-0.00017701069751441256)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.04637069702148437), np.float64(-2.220446049250313e-16)], [np.float64(-0.26500778198242186), np.float64(-1.7888543819998315)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.4328775119781494), np.float64(-2.1908902300206647)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.04547350883483887), np.float64(-0.0007206925343541948)], [np.float64(-0.22193310260772706), np.float64(-3.0044190474501162)], [np.float64(-0.05327143669128418), np.float64(-0.007045807375304747)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.036178874969482425), np.float64(-2.7755575615628914e-16)], [np.float64(-0.6688863945007324), np.float64(-4.47213595499958)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.12894299268722534), np.float64(-1.7697607177405108)], [np.float64(-0.04949725389480591), np.float64(-0.0027961826203616313)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.040123021602630614), np.float64(-0.0010984391936611493)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.04965136766433716), np.float64(-0.0029542215650726344)]]], 'normalized_truth_values': [[[np.float64(0.4826592402703363), np.float64(0.49536838508064834)], [np.float64(0.48955798540383966), np.float64(0.4999979203272089)], [np.int64(0), np.int64(0)], [np.float64(0.49003258686838874), np.float64(0.49999999999999994)], [np.float64(0.4900992343048353), np.float64(0.49999999999999994)], [np.float64(0.47384150335347125), np.float64(0.49918622819060293)], [np.float64(0.49080943697267315), np.float64(0.49995574732573694)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4884094025522137), np.float64(0.49999999999999994)], [np.float64(0.4341330853696781), np.float64(0.14321323703222516)], [np.int64(0), np.int64(0)], [np.float64(0.3934394180219202), np.float64(0.10057153747411847)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4886335813850515), np.float64(0.49981982687420995)], [np.float64(0.444743340731696), np.float64(0.047226634166177665)], [np.float64(0.48668528943640055), np.float64(0.4982385554431761)], [np.int64(0), np.int64(0)], [np.float64(0.4909562676894978), np.float64(0.49999999999999994)], [np.float64(0.3387462405274408), np.float64(0.011293882208110636)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.46780884111672233), np.float64(0.14557208854512196)], [np.float64(0.4876282123072851), np.float64(0.4993009548003746)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.48997059005611165), np.float64(0.49972539022919604)], [np.int64(0), np.int64(0)], [np.float64(0.48758970752722197), np.float64(0.49926144514587195)]]]}}}