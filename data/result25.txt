{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(-0.39720557851238747)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(-3.167131441129094)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ['Susan Abigail Sarandon, better known as Susan Sarandon, was born on October 4, 1946, in Edison, New Jersey. She is the daughter of Lenora Marie (née Criscione) and Phillip Leslie Tomalin, an advertising executive, television producer, and former nightclub singer. She has four brothers—Philip Jr., Terry (deceased May 19, 2016), Tim, and O\'Brian—and four sisters—Meredith, Bonnie, Amanda, and Missy.\n\nSarandon\'s paternal ancestry includes English, Irish, and Welsh heritage, with her English ancestors from Hackney in London and her Welsh ancestors from Bridgend. Her maternal side has Italian roots, specifically from the regions of Tuscany and Sicily. She was raised Roman Catholic and attended Roman Catholic schools.\n\nSarandon began her acting career in the 1970 film "Joe" and later appeared in the soap opera "All My Children" before gaining wider recognition. She has received numerous accolades throughout her career, including an Academy Award, a British Academy Film Award, and a Screen Actors Guild Award. She has also been nominated for six Primetime Emmy Awards and nine Golden Globe Awards.\n\nBeyond her acting, Sarandon is known for her social and political activism. She has supported various causes and has been a UNICEF Goodwill Ambassador since 1999. In 2006, she received the Action Against Hunger Humanitarian Award. Her contributions to cinema have been widely recognized, earning her the Lifetime Achievement Award at the 2009 Stockholm International Film Festival, induction into the New Jersey Hall of Fame in 2010, and the Outstanding Artistic Life Award for her Outstanding Contribution to World Cinema at the 2011 Shanghai International Film Festival. In 2013, she was invited to inaugurate the 44th International Film Festival of India in Goa, and in 2015, she received the Goldene Kamera international lifetime achievement award.\n\nSarandon remains an influential figure in both the entertainment industry and social advocacy.'], 'claims': [['Susan Abigail Sarandon, better known as Susan Sarandon, was born on October 4, 1946, in Edison, New Jersey.', 'Susan Sarandon is the daughter of Lenora Marie (née Criscione) and Phillip Leslie Tomalin.', 'Phillip Leslie Tomalin is an advertising executive, television producer, and former nightclub singer.', "Susan Sarandon has four brothers: Philip Jr., Terry, Tim, and O'Brian.", "Terry, one of Susan Sarandon's brothers, died on May 19, 2016.", 'Susan Sarandon has four sisters: Meredith, Bonnie, Amanda, and Missy.', "Sarandon's paternal ancestry includes English heritage.", "Sarandon's paternal ancestry includes Irish heritage.", "Sarandon's paternal ancestry includes Welsh heritage.", "Sarandon's English ancestors were from Hackney in London.", "Sarandon's Welsh ancestors were from Bridgend.", "Sarandon's maternal side has Italian roots.", "Sarandon's maternal Italian roots are from the regions of Tuscany and Sicily.", 'Sarandon was raised Roman Catholic.', 'Sarandon attended Roman Catholic schools.', "Sarandon began her acting career in the 1970 film 'Joe'.", "Sarandon later appeared in the soap opera 'All My Children'.", 'Sarandon has received an Academy Award.', 'Sarandon has received a British Academy Film Award.', 'Sarandon has received a Screen Actors Guild Award.', 'Sarandon has been nominated for six Primetime Emmy Awards.', 'Sarandon has been nominated for nine Golden Globe Awards.', 'Susan Sarandon is known for her social and political activism.', 'Susan Sarandon has supported various causes.', 'Susan Sarandon has been a UNICEF Goodwill Ambassador since 1999.', 'In 2006, Susan Sarandon received the Action Against Hunger Humanitarian Award.', "Susan Sarandon's contributions to cinema have been widely recognized.", 'Susan Sarandon earned the Lifetime Achievement Award at the 2009 Stockholm International Film Festival.', 'Susan Sarandon was inducted into the New Jersey Hall of Fame in 2010.', 'Susan Sarandon received the Outstanding Artistic Life Award for her Outstanding Contribution to World Cinema at the 2011 Shanghai International Film Festival.', 'In 2013, Susan Sarandon was invited to inaugurate the 44th International Film Festival of India in Goa.', 'In 2015, Susan Sarandon received the Goldene Kamera international lifetime achievement award.', 'Sarandon remains an influential figure in the entertainment industry.', 'Sarandon remains an influential figure in social advocacy.']], 'claim_correctness': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], 'question_text': ['Tell me a bio of Susan Sarandon.'], 'context': ['{\n  "id" : "5233796",\n  "contents" : "\\"Susan Sarandon\\"\\nSusan Sarandon Susan Abigail Tomalin, better known as Susan Sarandon (born October 4, 1946) is an American actress and activist. She has received an Academy Award, a British Academy Film Award, and a Screen Actors Guild Award, and has been nominated for six Primetime Emmy Awards and nine Golden Globe Awards. She is known for her social and political activism for a variety of causes. She was appointed a UNICEF Goodwill Ambassador in 1999 and received the Action Against Hunger Humanitarian Award in 2006. Sarandon began her career in the 1970 film \\"\\"Joe\\"\\", before appearing in the soap opera \\"\\"A"\n}\n{\n  "id" : "19635941",\n  "contents" : "\\"Susan Sarandon\\"\\nLenora Marie (née Criscione; b. 1923) and Phillip Leslie Tomalin (1917–1999), an advertising executive, television producer, and one-time nightclub singer. She has four brothers, Philip Jr., Terry (deceased May 19, 2016), Tim and O\'Brian, and four sisters, Meredith, Bonnie, Amanda, and Missy. Her father was of English, Irish, and Welsh ancestry, his English ancestors being from Hackney in London and his Welsh ancestors being from Bridgend. On her mother\'s side, she is of Italian descent, with ancestors from the regions of Tuscany and Sicily. Sarandon was raised Roman Catholic and attended Roman Catholic schools. She grew up in Edison, New"\n}\n{\n  "id" : "19635960",\n  "contents" : "\\"Susan Sarandon\\"\\nreceived the Lifetime Achievement Award at the 2009 Stockholm International Film Festival, was inducted into the New Jersey Hall of Fame in 2010, and received the Outstanding Artistic Life Award for her Outstanding Contribution to World Cinema at the 2011 Shanghai International Film Festival. In 2013, she was invited to inaugurate the 44th International Film Festival of India (IFFI) in Goa. In 2015, Sarandon received the Goldene Kamera international lifetime achievement award. Susan Sarandon Susan Abigail Tomalin, better known as Susan Sarandon (born October 4, 1946) is an American actress and activist. She has received an Academy Award, a British"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x0000019208A0E9E0>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x0000019208B59510>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000019208924C70>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000019208B68CA0>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.042840986251831054), np.float64(-0.0004964147845384903)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.03813847541809082), np.float64(-0.0001637215323734842)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.042990612983703616), np.float64(-0.00046535096560096356)], [np.float64(-0.03941670417785645), np.float64(-0.00030953247831699837)], [np.float64(-0.18024959087371825), np.float64(-1.8453074993965424)], [np.float64(-0.04561232566833496), np.float64(-0.0005885780949858588)], [np.float64(-0.5705817079544068), np.float64(-3.7632832929028512)], [np.float64(-0.2568840098381042), np.float64(-1.788685580794224)], [np.float64(-0.04772801399230957), np.float64(-2.220446049250313e-16)], [np.float64(-0.051233925819396973), np.float64(-0.00031679244243271176)], [np.float64(-0.0490778923034668), np.float64(-2.220446049250313e-16)], [np.float64(-0.0734023380279541), np.float64(-0.01786628491125941)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.040352606773376466), np.float64(-0.0011138284603052218)], [np.float64(-0.0674078369140625), np.float64(-0.010409357235171113)], [np.float64(-0.041466283798217776), np.float64(-0.0018576514490922236)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.05007457733154297), np.float64(-0.0020327912267148474)], [np.float64(-0.11309768199920654), np.float64(-0.01773669310300413)], [np.float64(-0.6096912384033203), np.float64(-4.472135954999579)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.044156761169433595), np.float64(-0.0008226974329221814)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.05050612211227417), np.float64(-0.004736998894184552)], [np.float64(-0.07989452838897705), np.float64(-0.017505127594895642)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.48929139122412935), np.float64(0.4998758963064139)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4904665366870692), np.float64(0.49995906961699804)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4892540017594809), np.float64(0.4998836622606992)], [np.float64(0.4901470996075429), np.float64(0.4999226168810386)], [np.float64(0.45505921330633015), np.float64(0.13642479231492388)], [np.float64(0.48859889516596916), np.float64(0.49985285548050146)], [np.float64(0.36110260961407376), np.float64(0.022681049785566742)], [np.float64(0.4361298412812147), np.float64(0.14323395072874587)], [np.float64(0.48807026104159507), np.float64(0.49999999999999994)], [np.float64(0.48719431957466336), np.float64(0.4999208018900541)], [np.float64(0.4877329890592332), np.float64(0.49999999999999994)], [np.float64(0.48165765032065283), np.float64(0.49553354758072704)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.48991321698954204), np.float64(0.49972154291371185)], [np.float64(0.48315441889090127), np.float64(0.49739768418893116)], [np.float64(0.48963491419744265), np.float64(0.4995355872712793)], [np.int64(0), np.int64(0)], [np.float64(0.4874839708481331), np.float64(0.49949180236832086)], [np.float64(0.47175567937768875), np.float64(0.495565942966244)], [np.float64(0.3521296338305679), np.float64(0.011293882208110647)], [np.int64(0), np.int64(0)], [np.float64(0.4889626030603981), np.float64(0.49979432565337)], [np.int64(0), np.int64(0)], [np.float64(0.4873761528388884), np.float64(0.4988157524909128)], [np.float64(0.4800369856589685), np.float64(0.49562382984967174)]]]}}}