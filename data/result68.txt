{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.5)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.5)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ["It seems there may be a mix-up in the request. The provided documents contain information about Karolina Kaczorowska and Klaudia Kulon, but not about Klaudia Kaczorowska. Here are the bios of the individuals mentioned:\n\n### Karolina Kaczorowska\nKarolina Kaczorowska (née Mariampolska) was born on September 26, 1930, in Stanisławów. She is the widow of Ryszard Kaczorowski, the last President of the Republic of Poland in exile, and thus the last emigree First Lady (1989-1990). Her life has been marked by significant historical events and personal achievements:\n\n- **Early Life and Deportation**: During the Soviet occupation (1939-1941), she and her family were deported to Siberia.\n- **Education**: After being released, she attended Polish school in Uganda and graduated from the University of London.\n- **Career**: For many years, she worked as a schoolteacher and was active in the Polish exiled Scouting movement, where she met her future husband, Ryszard Kaczorowski.\n- **Family**: Karolina and Ryszard had two daughters and five grandchildren.\n- **Tragedy**: Mrs. Kaczorowska was originally supposed to attend the 70th anniversary of the Katyn massacre along with her husband, but health reasons prevented her from going. The entire Polish delegation, including her husband, former President Kaczorowski, were killed in the 2010 Polish Air Force Tu-154 crash.\n\n### Klaudia Kulon\nKlaudia Kulon, born on March 13, 1992, is a Polish chess player who holds the title of Woman Grandmaster. Her chess career includes several notable achievements:\n\n- **Early Beginnings**: Her chess career began at the age of 7 at the Koszalin University of Technology chess club.\n- **Notable Wins**:\n  - Won the U12 Girls' World Championship in 2004.\n  - Won the U14 Girls' World Championship in 2006.\n  - Won the Women's World University Championship in 2014.\n- **Polish Championships**: From 2002, Klaudia won several Polish Junior Championships in different age categories.\n- **International Competitions**: In 2008, she was the runner-up behind Nazí Paikidze in the European Youth Chess Championship under the relevant age category.\n\nIf you have additional details or a different individual in mind, please let me know!"], 'claims': [['The provided documents contain information about Karolina Kaczorowska.', 'The provided documents contain information about Klaudia Kulon.', 'The provided documents do not contain information about Klaudia Kaczorowska.', 'Karolina Kaczorowska (née Mariampolska) was born on September 26, 1930, in Stanisławów.', 'Karolina Kaczorowska is the widow of Ryszard Kaczorowski.', 'Ryszard Kaczorowski was the last President of the Republic of Poland in exile.', 'Karolina Kaczorowska was the last emigree First Lady from 1989 to 1990.', 'She and her family were deported to Siberia during the Soviet occupation from 1939 to 1941.', 'After being released, she attended a Polish school in Uganda.', 'She graduated from the University of London.', 'She worked as a schoolteacher for many years.', 'She was active in the Polish exiled Scouting movement.', 'She met her future husband, Ryszard Kaczorowski, in the Polish exiled Scouting movement.', 'Karolina and Ryszard had two daughters.', 'Karolina and Ryszard had five grandchildren.', 'Mrs. Kaczorowska was originally supposed to attend the 70th anniversary of the Katyn massacre along with her husband.', 'Health reasons prevented Mrs. Kaczorowska from attending the 70th anniversary of the Katyn massacre.', 'The entire Polish delegation was killed in the 2010 Polish Air Force Tu-154 crash.', "Mrs. Kaczorowska's husband, former President Kaczorowski, was part of the Polish delegation killed in the 2010 Polish Air Force Tu-154 crash.", 'Klaudia Kulon was born on March 13, 1992.', 'Klaudia Kulon is a Polish chess player.', 'Klaudia Kulon holds the title of Woman Grandmaster.', 'Her chess career began at the age of 7.', 'Her chess career began at the Koszalin University of Technology chess club.', "She won the U12 Girls' World Championship in 2004.", "She won the U14 Girls' World Championship in 2006.", "She won the Women's World University Championship in 2014.", 'Klaudia won several Polish Junior Championships in different age categories from 2002.', 'In 2008, she was the runner-up in the European Youth Chess Championship under the relevant age category.', 'Nazí Paikidze won the European Youth Chess Championship in 2008 under the relevant age category.']], 'claim_correctness': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1])], 'question_text': ['Tell me a bio of Klaudia Kaczorowska.'], 'context': ['{\n  "id" : "10812418",\n  "contents" : "\\"Karolina Kaczorowska\\"\\nKarolina Kaczorowska Karolina Kaczorowska (maiden name Mariampolska) (born September 26, 1930 in Stanisławów) is the widow of Ryszard Kaczorowski, the last President of the Republic of Poland in exile, and thus the last emigree First Lady (1989-1990). During the Soviet occupation (1939-1941) she was deported, along with her family, to Siberia. After being released, she attended Polish school in Uganda and graduated from the University of London. For a number of years, she worked as a schoolteacher and was active in the Polish exiled Scouting movement, where she met her future husband. They got married on July 19, 1952. Karolina"\n}\n{\n  "id" : "10056161",\n  "contents" : "\\"Klaudia Kulon\\"\\nKlaudia Kulon Klaudia Kulon (born 13 March 1992) is a Polish chess player who holds the title of Woman Grandmaster. She won the U12 Girls\' World Championship in 2004, the U14 Girls\' World Championship in 2006, the Women\'s World University Championship in 2014. Chess career began at the age of 7 in Koszalin University of Technology chess club and, in a short time, reaching a number of successes in chess. From 2002 Klaudia several times won Polish Junior Championships in different age categories. In 2008, she was the runner up behind Nazí Paikidze in the European Youth Chess Championship under"\n}\n{\n  "id" : "12408073",\n  "contents" : "\\"Karolina Kaczorowska\\"\\nand Ryszard had 2 daughters, as well as 5 grandchildren. Mrs Kaczorowska was originally supposed to attend the 70th anniversary of the Katyn massacre along with her husband, but health reasons prevented her from going. The entire Polish delegation, including former President Kaczorowski, were killed in the 2010 Polish Air Force Tu-154 crash. Karolina Kaczorowska Karolina Kaczorowska (maiden name Mariampolska) (born September 26, 1930 in Stanisławów) is the widow of Ryszard Kaczorowski, the last President of the Republic of Poland in exile, and thus the last emigree First Lady (1989-1990). During the Soviet occupation (1939-1941) she was deported, along with"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x0000025C333729E0>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x0000025C334B9510>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000025C332B4C70>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x0000025C334CCCA0>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.045480537414550784), np.float64(-3.287505994420403e-05)], [np.float64(-0.17957363843917848), np.float64(-1.7748276898327882)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.07925252437591553), np.float64(-0.016716772802993385)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.040120935440063475), np.float64(-8.406097151331116e-05)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.09791637420654296), np.float64(-0.02231964155091315)], [np.float64(-0.08770829200744629), np.float64(-0.015578681800540617)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.32851505279541016), np.float64(-2.1908902300206647)], [np.float64(-0.3801046705245972), np.float64(-2.190890230020664)], [np.float64(-0.0433286190032959), np.float64(-1.6653345369377348e-16)], [np.float64(-0.27879390001297), np.float64(-2.063861901264877)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.5844819402694702), np.float64(-3.0852687320902428)], [np.float64(-0.2082160520553589), np.float64(-1.788189321525535)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.05273523330688477), np.float64(-2.220446049250313e-16)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.float64(0.48863182514832443), np.float64(0.49999178123501464)], [np.float64(0.4552268412969919), np.float64(0.14494298535857789)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.48019723285782584), np.float64(0.4958209041196654)], [np.int64(0), np.int64(0)], [np.float64(0.4899711113869192), np.float64(0.49997898475713404)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.47554044572817195), np.float64(0.4944203212443879)], [np.float64(0.47808697280731766), np.float64(0.4961054083161463)], [np.int64(0), np.int64(0)], [np.float64(0.4186019774926459), np.float64(0.10057153747411847)], [np.float64(0.4061016520561351), np.float64(0.10057153747411854)], [np.float64(0.48916953959391624), np.float64(0.49999999999999994)], [np.float64(0.4307494923098854), np.float64(0.11265918913187425)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.35790195262196334), np.float64(0.04371901173147973)], [np.float64(0.4481332373692662), np.float64(0.1432948614496758)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4868192461767111), np.float64(0.49999999999999994)]]]}}}