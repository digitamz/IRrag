{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.5)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.5)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ["Mike Adenuga, born Mike Adenuga Michael Adeniyi Agbolade Ishola Adenuga Jr., GCON, on April 29, 1953, is a prominent Nigerian billionaire businessman and the second-richest person in Nigeria. His father, Oloye Michael Agbolade Adenuga Sr., was a school teacher, and his mother, Omoba Juliana Oyindamola Adenuga (née Onashile), was a businesswoman of royal Ijebu descent from Okesopin, Ijebu Igbo.\n\nAdenuga received his early education at Ibadan Grammar School and Comprehensive High School, Aiyetoro, where he completed his Higher School Certificate (HSC). To support his university education, he worked as a taxi driver. He later earned degrees in Business Administration from Northwestern Oklahoma State University and Pace University in New York.\n\nAdenuga's business career began in earnest in 1990 when he received a drilling license. The following year, his company, Consolidated Oil (now known as Conoil), struck oil in the shallow waters of Southwestern Ondo State, marking the first such achievement for an indigenous oil company in Nigeria. This success laid the foundation for his substantial wealth and influence in the Nigerian business landscape.\n\nIn addition to his oil ventures, Adenuga is the founder and chairman of Globacom, Nigeria's second-largest telecom operator, which also has a presence in Ghana and Benin. He has significant stakes in other companies, including Equitorial Trust Bank.\n\nAs of 2017, Forbes estimated Adenuga's net worth at $5.8 billion, making him the second-wealthiest Nigerian after Aliko Dangote. His business acumen and entrepreneurial spirit have made him a key figure in Nigeria's economic development."], 'claims': [['Mike Adenuga was born on April 29, 1953.', 'Mike Adenuga is a prominent Nigerian billionaire businessman.', 'Mike Adenuga is the second-richest person in Nigeria.', "Mike Adenuga's father, Oloye Michael Agbolade Adenuga Sr., was a school teacher.", "Mike Adenuga's mother, Omoba Juliana Oyindamola Adenuga (née Onashile), was a businesswoman of royal Ijebu descent.", "Mike Adenuga's mother is from Okesopin, Ijebu Igbo.", 'Adenuga received his early education at Ibadan Grammar School.', 'Adenuga completed his Higher School Certificate (HSC) at Comprehensive High School, Aiyetoro.', 'Adenuga worked as a taxi driver to support his university education.', 'Adenuga earned a degree in Business Administration from Northwestern Oklahoma State University.', 'Adenuga earned a degree in Business Administration from Pace University in New York.', "Adenuga's business career began in earnest in 1990 when he received a drilling license.", "In 1991, Adenuga's company, Consolidated Oil, struck oil in the shallow waters of Southwestern Ondo State.", 'Consolidated Oil is now known as Conoil.', 'Striking oil in the shallow waters of Southwestern Ondo State was the first such achievement for an indigenous oil company in Nigeria.', "This success laid the foundation for Adenuga's substantial wealth and influence in the Nigerian business landscape.", 'Adenuga is the founder and chairman of Globacom.', "Globacom is Nigeria's second-largest telecom operator.", 'Globacom has a presence in Ghana and Benin.', 'Adenuga has significant stakes in other companies, including Equitorial Trust Bank.', "As of 2017, Forbes estimated Adenuga's net worth at $5.8 billion.", 'Adenuga is the second-wealthiest Nigerian after Aliko Dangote as of 2017.', "Adenuga's business acumen and entrepreneurial spirit have made him a key figure in Nigeria's economic development."]], 'claim_correctness': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1])], 'question_text': ['Tell me a bio of Mike Adenuga.'], 'context': ['{\n  "id" : "8823464",\n  "contents" : "\\"Mike Adenuga\\"\\nmother, Omoba Juliana Oyindamola Adenuga (née Onashile, of Okesopin, Ijebu Igbo), was a businesswoman of royal Ijebu descent. Adenuga received his secondary school education at Ibadan Grammar School, Ibadan, and Comprehensive High School, Aiyetoro, for his Higher School Certificate (HSC). He worked as a taxi driver to help fund his university education. He graduated from Northwestern Oklahoma State University and Pace University, New York, with degrees in Business Administration. In 1990, he received a drilling license and in 1991, his Consolidated Oil struck oil in the shallow waters of Southwestern Ondo State, the first indigenous oil company to do so"\n}\n{\n  "id" : "8823463",\n  "contents" : "\\"Mike Adenuga\\"\\nMike Adenuga Michael Adeniyi Agbolade Ishola Adenuga Jr, GCON (born 29 April 1953) is a Nigerian billionaire businessman, and the second-richest person in Nigeria. His company Globacom is Nigeria\'s second-largest telecom operator, and has a presence in Ghana and Benin. He owns stakes in the Equitorial Trust Bank and the oil exploration firm Conoil (formerly \\"\\"Consolidated Oil Company\\"\\"). \\"\\"Forbes\\"\\" has estimated his net worth at $5.8 billion as of 2017, which makes him the second-wealthiest Nigerian behind Aliko Dangote, with a net worth of $14.1 billion. His father, the Oloye Michael Agbolade Adenuga Sr, was a school teacher while his"\n}\n{\n  "id" : "20935020",\n  "contents" : "\\"Paddy Adenuga\\"\\nPaddy Adenuga Mike Agbolade Adeniyi Ishola Paddy Adenuga or Paddy Adenuga (born June 21, 1984) is a Nigerian businessman and screenwriter born in London, England. He is the son of Nigerian billionaire Mike Adenuga. Born in London, England to Mike Adenuga Jr. from Ijebu Igbo in Ogun State and Emelia Marquis of Calabar descent from Cross River State. At the age of 10, Paddy Adenuga relocated from Lagos, Nigeria to Harlingen, Texas in the Rio Grande Valley to attend the Marine Military Academy. After 3 years at Marine Military Academy, he attended The Tenney School in Houston, Texas. By early"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x00000229AD35FE80>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x00000229AD3AD360>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x00000229AD35FF70>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x00000229AD3CA7D0>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.28755951642990113), np.float64(-2.866546881802851)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.5280616092681885), np.float64(-3.085317421020581)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.03760864019393921), np.float64(-0.0018041750418148483)], [np.float64(-0.053366231918334964), np.float64(-5.551115123125783e-17)], [np.float64(-0.04093242645263672), np.float64(-0.0013126514825096414)], [np.float64(-0.06858352422714234), np.float64(-0.013762713095428425)], [np.float64(-0.05536000728607178), np.float64(-0.008696810628428309)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.42860144255377747), np.float64(0.05383226360746652)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.37096910025499535), np.float64(0.0437169762064681)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.4905989480037478), np.float64(0.49954895636189367)], [np.float64(0.486661607464988), np.float64(0.5)], [np.float64(0.489768321913912), np.float64(0.4996718371764928)], [np.float64(0.48286083653991124), np.float64(0.4965593760340049)], [np.float64(0.48616353175174204), np.float64(0.49782581104651963)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)]]]}}}