{'sample_level_eval_list': {'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}, 'claim_check_methods_0_truth_method_1': {'f1': {'values': [0.0], 'mean': np.float64(0.0), 'max': np.float64(0.0), 'min': np.float64(0.0), 'std': np.float64(0.0)}}}, 'dataset_level_eval_list': {'claim_check_methods_0_truth_method_0': {'prr': np.float64(0.5)}, 'claim_check_methods_0_truth_method_1': {'prr': np.float64(0.5)}}, 'output_dict': {'previous_context': [{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], 'user_prompt': 'Context: {context}\nQuestion: {question}', 'generation': ["Ivan Toms (11 July 1952 – 25 March 2008) was a prominent South African physician and anti-Apartheid activist. Here is a brief biography:\n\n- **Birth and Early Life**: Born on 11 July 1952 in Cape Town, South Africa. He attended Glenwood High School in Durban from 1965 to 1969 and served as deputy Head Prefect in his final year.\n\n- **Education**: Received his medical degree from the University of Cape Town in 1976.\n\n- **Activism**: Ivan Toms was a significant figure in the anti-Apartheid movement and an advocate against conscription. He used his medical knowledge and influence to support and empower marginalized communities.\n\n- **Career**:\n  - **1993**: Became the director of the Students' Health and Welfare Centres Organisation (SHAWCO), a non-governmental organization that runs medical clinics staffed by students in underprivileged areas.\n  - **1996**: Appointed as the Health Director for the City of Cape Town, South Africa.\n  - **2006**: Awarded the Order of the Baobab by South African President Thabo Mbeki for his significant contributions and his stance against Apartheid.\n\n- **Legacy**: Ivan Toms' legacy is commemorated through the Ivan Toms Centre for Men's Health in Woodstock, Cape Town, which focuses on the prevention, diagnosis, and treatment of sexually transmitted infections.\n\n- **Death**: Ivan Toms passed away on 25 March 2008. His funeral, attended by hundreds of people including Archbishop Desmond Tutu, was held at St. George's Cathedral in Cape Town.\n\nIvan Toms' dedication to social justice and healthcare for the marginalized has left a lasting impact on South African society."], 'claims': [['Ivan Toms was born on 11 July 1952.', 'Ivan Toms died on 25 March 2008.', 'Ivan Toms was a South African physician.', 'Ivan Toms was an anti-Apartheid activist.', 'He was born on 11 July 1952.', 'He was born in Cape Town, South Africa.', 'He attended Glenwood High School in Durban from 1965 to 1969.', 'He served as deputy Head Prefect in his final year at Glenwood High School.', 'He received his medical degree from the University of Cape Town in 1976.', 'Ivan Toms was a significant figure in the anti-Apartheid movement.', 'Ivan Toms was an advocate against conscription.', 'Ivan Toms used his medical knowledge to support marginalized communities.', 'Ivan Toms used his influence to empower marginalized communities.', "In 1993, the individual became the director of the Students' Health and Welfare Centres Organisation (SHAWCO).", 'SHAWCO is a non-governmental organization that runs medical clinics staffed by students in underprivileged areas.', 'In 1996, the individual was appointed as the Health Director for the City of Cape Town, South Africa.', 'The Order of the Baobab was awarded in 2006.', 'The Order of the Baobab was awarded by South African President Thabo Mbeki.', 'The Order of the Baobab was awarded for significant contributions and a stance against Apartheid.', "Ivan Toms' legacy is commemorated through the Ivan Toms Centre for Men's Health.", "The Ivan Toms Centre for Men's Health is located in Woodstock, Cape Town.", "The Ivan Toms Centre for Men's Health focuses on the prevention, diagnosis, and treatment of sexually transmitted infections.", 'Ivan Toms passed away on 25 March 2008.', "Ivan Toms' funeral was attended by hundreds of people.", "Archbishop Desmond Tutu attended Ivan Toms' funeral.", "Ivan Toms' funeral was held at St. George's Cathedral in Cape Town.", "Ivan Toms' dedication to social justice has left a lasting impact on South African society.", "Ivan Toms' dedication to healthcare for the marginalized has left a lasting impact on South African society."]], 'claim_correctness': [array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1])], 'question_text': ['Tell me a bio of Ivan Toms.'], 'context': ['{\n  "id" : "17260013",\n  "contents" : "\\"Ivan Toms\\"\\nIvan Toms Ivan Toms (11 July 1952 – 25 March 2008) was a South African physician, who battled the Apartheid era government as a prominent anti-Apartheid and anti-conscription activist. At the time of his death in 2008, Toms was serving as the Director of Health for the city of Cape Town, South Africa. Ivan Toms was born in Cape Town on 11 July 1952. He received his medical degree from the University of Cape Town in 1976. Ivan attended Glenwood High School in Durban from 1965 to 1969. He was deputy Head Prefect in 1969. A eulogy to Ivan can"\n}\n{\n  "id" : "17260020",\n  "contents" : "\\"Ivan Toms\\"\\nby hundreds of people including Archbishop Tutu, was held at St. George\'s Cathedral in Cape Town. Ivan Toms Ivan Toms (11 July 1952 – 25 March 2008) was a South African physician, who battled the Apartheid era government as a prominent anti-Apartheid and anti-conscription activist. At the time of his death in 2008, Toms was serving as the Director of Health for the city of Cape Town, South Africa. Ivan Toms was born in Cape Town on 11 July 1952. He received his medical degree from the University of Cape Town in 1976. Ivan attended Glenwood High School in Durban"\n}\n{\n  "id" : "17260018",\n  "contents" : "\\"Ivan Toms\\"\\nbecame director of the Students\' Health and Welfare Centres Organisation in 1993, which is a non-governmental organization which runs medical clinics staffed by students in poor areas. He continued to work for non-governmental charities until 1996, when he became the Health Director in the City of Cape Town. His name is remembered in the Ivan Toms Centre for Men\'s Health (Cape Town) in Woodstock, Cape Town, which works in the prevention, diagnosis, and treatment of sexually transmitted infections. South African President Thabo Mbeki awarded Ivan Toms with the Order of the Baobab in 2006 for his stance against Apartheid and"\n}'], 'claim_check_methods': ['QuestionAnswerGeneration'], 'claim_check_methods_0': {'name': 'Claim Check Method by Generating Questions and Answers.\nQuestion generation model: openrouter/qwen/qwen-2.5-72b-instruct\nNumber of questions to be generated for a stament: 2\nNumber of trials to generate an answer that entails with the claim: 2\nEntailment check model: <class \'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification\'>\n\nQuestion generation instruction for the first claim:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nQuestion generation instruction for claims with preceeding text:\n  [{\'role\': \'system\', \'content\': "You are an expert assistant skilled at generating focused and contextually relevant questions from claims. Your task is to create a question such that the answer would align closely with the provided claim. To ensure the question is precise and relevant, consider the context provided by the original question. Study the examples below from a variety of topics and follow the same pattern.\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 explores the theme of government surveillance. \\nQuestion: What theme does George Orwell\'s novel 1984 explore?   \\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: George Orwell\'s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens\' lives. \\nQuestion: How does George Orwell\'s novel 1984 reflect the theme of totalitarian control, as commonly explored in 20th-century dystopian literature?\\n\\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\\nClaim: The novel 1984 is written by George Orwell.\\nQuestion: Who has written the novel 1984?\\n\\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\\nClaim: Artificial intelligence enables better decision-making through data analysis.\\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\\n\\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\\nClaim: Stock market speculation contributed to the Great Depression.\\nQuestion: Did stock market speculation contribute to the Great Depression?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\\nQuestion: What is Abraham Lincoln\'s most significant historical contribution?\\n\\nOriginal Question: Who is Abraham Lincoln?\\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\\nQuestion: When did Abraham Lincoln serve as the president of the United States?\\n\\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim,without adding explanations, introductions, or conversational responses."}, {\'role\': \'user\', \'content\': \'Original question: {question}\\nClaim: {claim}\\nQuestion: \'}]\n\nAnswer generation instruction:\n    [{\'role\': \'system\', \'content\': \'You are a helpful assistant. Give a single claim answer to given question. Don\\\\‘t provide any additional information. Just answer the question with a brief sentence in a single claim.\'}, {\'role\': \'user\', \'content\': \'{question}\'}]\n\nTruth methods to assign a score the question(s):\n   [<TruthTorchLM.truth_methods.matrix_degree_uncertainty.MatrixDegreeUncertainty object at 0x000002522171FE20>, <TruthTorchLM.truth_methods.eccentricity_uncertainty.EccentricityUncertainty object at 0x00000251DB70BEE0>]', 'truth_methods': ['MatrixDegreeUncertainty', 'EccentricityUncertainty'], 'truth_methods_name': ['MatrixDegreeUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x000002522171FF10>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'temperature\': 3.0, \'batch_generation\': True}', 'EccentricityUncertainty with {\'normalizer\': <TruthTorchLM.normalizers.sigmoid_normalizer.SigmoidNormalizer object at 0x000002522176D300>, \'model_for_entailment\': DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 1024)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n), \'tokenizer_for_entailment\': DebertaTokenizer(name_or_path=\'microsoft/deberta-large-mnli\', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side=\'right\', truncation_side=\'right\', special_tokens={\'bos_token\': \'[CLS]\', \'eos_token\': \'[SEP]\', \'unk_token\': \'[UNK]\', \'sep_token\': \'[SEP]\', \'pad_token\': \'[PAD]\', \'cls_token\': \'[CLS]\', \'mask_token\': \'[MASK]\'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50264: AddedToken("[MASK]", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}\n), \'number_of_generations\': 5, \'method_for_similarity\': \'semantic\', \'eigen_threshold\': 0.9, \'temperature\': 3.0, \'batch_generation\': True}'], 'truth_values': [[[np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.5160109782218933), np.float64(-2.8840768933650263)], [np.float64(-0.1369560766220093), np.float64(-1.7468314595787813)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.31169905662536623), np.float64(-1.7888543819998313)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.4346584725379944), np.float64(-2.884299497010163)], [np.float64(-0.052956416606903076), np.float64(-0.004075015829055784)], [np.float64(-0.03670151710510254), np.float64(-0.00047333611604116754)], [np.float64(-0.09302514314651489), np.float64(-0.030391988349984078)], [np.float64(-0.0737949800491333), np.float64(-0.022909135797787694)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.35681737661361695), np.float64(-2.189255033229623)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-10000000000.0), np.float64(-10000000000.0)], [np.float64(-0.6343722963333129), np.float64(-3.77872668801008)], [np.float64(-0.05426951169967651), np.float64(-0.006671453606469202)], [np.float64(-10000000000.0), np.float64(-10000000000.0)]]], 'normalized_truth_values': [[[np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.3737854724901922), np.float64(0.05294633566348966)], [np.float64(0.4658143989963005), np.float64(0.1484472894178425)], [np.int64(0), np.int64(0)], [np.float64(0.4227000726873437), np.float64(0.1432132370322252)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.3930144817266624), np.float64(0.05293517475492419)], [np.float64(0.48676398893970374), np.float64(0.4989812474524985)], [np.float64(0.4908256505223726), np.float64(0.4998816659731991)], [np.float64(0.4767604707455755), np.float64(0.4924025876971914)], [np.float64(0.4815596226237958), np.float64(0.4942729665242293)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.41173020705216545), np.float64(0.10071954895201102)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.int64(0), np.int64(0)], [np.float64(0.3465197950237115), np.float64(0.022341233601662853)], [np.float64(0.48643595095873166), np.float64(0.4983321427845012)], [np.int64(0), np.int64(0)]]]}}}